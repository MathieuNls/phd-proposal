%!TEX root = research_proposal.tex

\chapter{Related Work\label{chap:relwork}}

\section{Crash reproduction}

In his Ph.D thesis \cite{Chen2013}, Chen proposed an approach named STAR (Stack Trace based Automatic crash Reproduction).
Using only the crash stack, STAR starts from the crash point and goes backward towards the entry point of the program.
During the backward process, STAR computes the required condition to reach the crash point using an SMT (Satisfiability Modulo Theories) solver named Yices \cite{Dutertre2006}.
The objects that satisfy the required conditions are generated and orchestrated inside a JUnit test case. The test is run and the resulting crash stack is compared to the original one. If both match, the bug is said to be reproduced. When applied to different systems, STAR achieved 60\% accuracy. \\

Jaygarl et al. \cite{Jaygarl} created OCAT (Object Capture based Automated Testing).
The authors’ approach starts by capturing objects created by the program when it runs on-field in order to provide them in an automated test process. Indeed the coverage of automated tests is often low due to the lack of correctly constructed objects.
Also, the objects can be mutated by means of evolutionary algorithms. These mutations target primitive fields in order to create even more objects and therefore improve the code coverage once more.
While not targeting the reproduction of a bug, OCAT is a well-known approach and was used as the main mechanism for bug reproduction. \\

Narayanasamy et al. \cite{Narayanasamy2005} proposed BugNet, a tool that continuously records program execution for deterministic replay debugging. According to the authors, the size of the recorded data needed to reproduce a bug with high accuracy is around 10MB. This recording is then sent to the developers and allows the deterministic replay of a bug. The authors argued that, with nowadays Internet bandwidth, the size of the recording is not an issue during the transmission of the recorded data, however, the instrumentation of the system is problematic since it slows down considerably the execution.\\

Jin et al. \cite{Jin2012} proposed BugRedux for reproducing field failures for in-house debugging. The tool aims to synthesize in-house executions that mimic field failures. To do so, the authors use several types of data collected in the field such as stack traces, crash stacks, and points of failure.
The data that successfully reproduced the field crash is sent to software developers to fix the bug. \\

Based on the success of BugRedux, the authors built F3 (Fault localization for Field Failures) \cite{Jin2013}.
F3 performs many executions of a program on top of BugRedux in order to cover different paths leading to the fault. It then generates many ‘pass’ and ‘fail’ paths which can lead to a better understanding of the bug. They also use grouping, profiling and filtering, to improve the fault localization process.\\

While being close to our approach, BugRedux and F3 may require the call sequence and/or the complete execution trace in order to achieve bug reproduction. When using only the crash traces (referred to as call stack at crash time in their paper), the success rate of BugRedux significantly drops to 37.5\% (6/16). The call sequence and the complete execution trace required to reach 100\% of bug reproduction can only be obtained through instrumentation and with an overhead ranging from 1\% to 1066\%.\\

Clause et al. \cite{Clause2007} proposed a technique for enabling and supporting debugging of field failures.
They record the execution of the program on the client side and propose to compress the generated data to the minimal required size to ensure that the reproduction is feasible.
This compression is also performed on the client side. Moreover, the authors keep traces of all accessed documents in the operating system and also compress/reduce them to the minimal.
Overall, they are able to reproduce on-field bug using a file weighting ≈70Kb. The minimal execution paths triggering the failure are then sent to the developers who can replay the execution on a sandbox, simulating the client’s environment. While efficient, this approach suffers from severe security and privacy issues. \\

RECORE (REconstructing CORE dumps) is a tool proposed by Rossler et al. \cite{Rossler2013}.
It instruments Java bytecode to wrap every method in a try and catch block while keeping a quasi-null overhead.
The tool starts from the core dump and tries (with evolutionary algorithms) to reproduce the same dump by executing the programs many times.
The set of inputs responsible for the failure is generated when the generated dump matches the collected one.
ReCrash \cite{Artzi2008} is a tool that aims to make software failures reproducible by preserving object states.
It uses an in-memory stack, which contains every argument and object clone of the real execution in order to reproduce a crash via the automatic generation of unit test cases.
Unit test cases are used to provide hints to the developers on the buggy code. This approach suffers from overhead when they record everything (between 13\% to 64\% in some cases).
The authors also propose an alternative in which they record only the methods surrounding the crash.
For this to work, the crash has to occur at least once so they could use the information causing the crash to identify the methods surrounding it when (and if) it appears. \\

JRapture \cite{Steven2000} is a capture/replay tool for observation-based testing.
The tool captures execution of Java programs to replay it in-house.  To capture the execution of a Java program, the authors used their own version of the Java Virtual Machine (JVM) and employ a lightweight, transparent capture process. Using their own JVM allows one to capture any interactions between a Java program and the system, including GUI, file, and console inputs, and on replay, it presents each thread with exactly the same input sequence it saw during capture.
Unfortunately, they have to make their customer use their own JVM in order to support their approach, which limits the generalization of the approach to mass-market software.\\

Finally, Zamfir et al. \cite{Parnin2011} proposed ESD, an execution synthesis approach which automatically synthesizes failure execution using only the stack trace information. However, this stack trace is extracted from the core dump and may not always contain the components that caused the crash.\\

Except for STAR, approaches targeting the reproduction of field crashes require the instrumentation of the code or the running platform in order to save the stack call or the objects to successfully reproduce bugs. As we discussed earlier, instrumentation can cause a massive overhead (1\% to 1066\%) while running the system. In addition, data generated at run-time using instrumentation may contain sensitive information.

\section{Issue and source code relationships\label{rel:issue-rela}}

Researchers have been studying the relationships between issues and source code repositories since more than two decades now.
To the best of our knowledge the first ones who conduct this type of study on a significant scale were Perry and Stieg \cite{PerryDewayneE.1993}.
In these two decades, many aspects of these relationships have been studied in length.
For example, researchers  interested themselves in ameliorating the issues report itself by specified guidelines to make good report \cite{Bettenburg2008} and try to further simplify the existing models \cite{Herraiz2008}.

Then, we can find approaches on how long it will take for a issues to get fixed \cite{Bhattacharya2011,Zhang2013,Saha2014} and where it should be fixed \cite{Zhou2012,Kim2013a}.
With the rapidly increasing number of issues, the community also interested itself in prioritizing the issues report compared to one another \cite{Kim2011c} and partially do so by predicting the severity of a issues \cite{Lamkanfi2010}.

Finally, researchers proposed approaches to predict which issues will get reopened \cite{Zimmermann2012,Lo2013} which issues report is a duplicate of which other one \cite{Jalbert2008,Bettenburg2008a,Tian2012a}.

Another field of study consist in assigning these issues reports, if possible automatically to the right developers through triaging  \cite{Anvik2006,Jeong2009,Tamrawi2011a,Bortis2013}
and which locations are likely to yield new bugs \cite{Kim2006,Kim2007}.

\section{Crash Prediction}

Predicting crash, fault and bug is very large and popular research area.
The main goal behind the pletor of papers is to save on manpower---being to more expensive resource to build software---by directing their efforts on locations likely to contain a bug, fault or crash.

There are two distinct trends in crash, fault and bug prediction in the papers accepted to major venues such as MSR, ICSE, ICSME and ASE:  analysis and current version analysis.

In the  analysis, researchers extract and interpret information from  system.
The idea being that the files or locations that are the most frequently changed are more likely to contain a bug.
Additionally, some of these approaches also assume that locations linked to a previous bug are likely to be linked to a bug in the future.

On the other hand, approaches using only the current version to predict bugs assume that the current version, i.e. its design, call graph, quality metrics and more, will trigger the appearance of the bug in the future.
Consequently, they do no require the history and only need the current source-code.

In the remaining of this section, we will describe approaches belonging to the two families.

\subsubsection{Change logs approaches}
\label{subs:Change logs approaches}

Change logs based approaches rely on mining the historical data of the application and more particularly, the source code \textit{diffs}.
A source code \textit{diffs} contains two versions of the same code in one file.
Indeed, it contains the lines of code that have been deleted and the one that have been added.
It is worth noting that, \textit{diffs} files do not represent the concept of modified line.
Indeed, a modified line will be represented by a deletion and an addition.
Researchers mainly use five metrics when dealing with \textit{diffs} files :

\begin{itemize}
  \item Number of files: The number of modified files in a given commit
  \item Insertions: The number of added lines
  \item Deletions: The number of deleted lines
  \item Churns: The number of deletion lines immediately followed by an insertion which give an approximation of how many lines have been modified
  \item Hunks: The number of consecutive blocks of lines. This gives an approximation of how many distinct locations have been edited to accomplish a unit of work.
\end{itemize}

Naggapan \textit{et al.} studied the churns metric and how it can be connected to the apparition of new defect in a complex software system.
They established that relative churns are, in fact, a better metric than classical churn \cite{Nagappan} while studying Windows Server 2003.

Hassan, interested himself with the entropy of change, i.e. how complex the change is \cite{Hassan2009}.
Then, the complexity of the change, or entropy, can be used to predict bugs.
The more complex a change is, the more likely it is to bring the defect with it.
Hassan used its entropy metric, with success, on six different systems.
Prior to this work, Hassan, in collaboration with Holt proposed an approach that highlights the top ten most susceptible locations to have a bug using heuristics based on \textit{diffs} file metrics \cite{Hassan2005}.
Moreover, their heuristics also leverage the data of the bug tracking system.
Indeed, they use the past defect location to predict new ones.
The conclusion of these two approaches has been that recently modified and fixed locations where the most defect-prone and comparison with frequently modified.

Similarly to Hassan and Hold,  Ostrand \textit{et al.} predict future crash location by combining the data from change and past defect location \cite{Ostrand2005}.
The main difference between Hassan and Hold and Ostrand \textit{et al.} is that Ostrand \textit{et al.} validate their approach on industrial systems as they are members of the AT\&T lab while Hassan and Hold validated their approach on open-source system.
This proved that these metrics are relevant for open-source and industrial system.


%
% he bug cache approach by Kim et al. uses
% the same properties of recent changes and defects as the
% top ten list approach, but further assumes that faults occur
% in bursts [13]. The bug-introducing changes are identified
% from the SCM logs. Seven open-source systems were used
% to validate the findings (Apache, PostgreSQL, Subversion,
% Mozilla, JEdit, Columba, and Eclipse). Bernstein et al. use
% bug and change information in non-linear prediction models
% [12]. Six eclipse plugins were used to validate the approach.
% Single-version approaches assume that the current design
% and behavior of the program influences the presence of
% future defects. These approaches do not require the history
% of the system, but analyze its current state in more detail,
% using a variety of metrics. One standard set of metrics used
% is the Chidamber and Kemerer (CK) metrics suite [17].
% Basili et al. used the CK metrics on eight mediumsized
% information management systems based on the same
% requirements [1]. Ohlsson et al. used several graph metrics
% including McCabe’s cyclomatic complexity on an Ericsson
% telecom system [2]. El Emam et al. used the CK metrics
% in conjunction with Briand’s coupling metrics [3] to predict
% faults on a commercial Java system [4]. Subramanyam et
% al. used CK metrics on a commercial C++/Java system [5];
% Gyimothy et al. performed a similar analysis on Mozilla
% [6]. Nagappan and Ball estimated the pre-release defect
% density of Windows Server 2003 with a static analysis tool
% [7]. Nagappan et al. used a catalog of source code metrics
% to predict post release defects at the module level on five
% Microsoft systems, and found that it was possible to build
% predictors for one individual project, but that no predictor
% would perform well on all the projects [8]. Zimmermann et
% al. applied a number of code metrics on Eclipse [18].
% Other Approaches. Zimmermann and Nagappan used
% dependencies between binaries in Windows server 2003
% to predict defect [19]. Marcus et al. used a cohesion
% measurement based on LSI for defect prediction on
% several C++ systems, including Mozilla [20]. Neuhaus
% et al. used a variety of features of Mozilla (past bugs,
% package imports, call structure) to detect vulnerabilities [21].
% Observations We observe that both case studies and the
% granularity of approaches vary. Varying case studies make
% a comparative evaluation of the results difficult. Validations
% performed on industrial systems are not reproducible, because
% it is not possible to obtain the data that was used. There
% is also some variation among open-source case studies, as
% some approaches have more restrictive requirements than
% others. With respect to the granularity of the approaches,
% some of them predict defects at the class level, others
% consider files, while others consider modules or directories
% (subsystems), or even binaries. While some approaches
% predict the presence or absence of bugs for each component,
% others predict the amount of bugs affecting each component
% in the future, producing a ranked list of components.
% These observations explain the lack of comparison between
% approaches and the occasional diverging results when
% comparisons are performed. In the following, we present a
% benchmark to establish a common ground for comparison.
%
%
% —While many bug prediction algorithms have been
% developed by academia, they’re often only tested and verified
% in the lab using automated means. We do not have a strong
% idea about whether such algorithms are useful to guide human
% developers. We deployed a bug prediction algorithm across
% Google, and found no identifiable change in developer behavior.
% Using our experience, we provide several characteristics that bug
% prediction algorithms need to meet in order to be accepted by
% human developers and truly change how developers evaluate their
% code.
