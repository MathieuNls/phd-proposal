%!TEX root = research_proposal.tex

\chapter{Related Work\label{chap:relwork}}

\section{Crash reproduction}

In his Ph.D thesis \cite{Chen2013}, Chen proposed an approach named STAR (Stack Trace based Automatic crash Reproduction).
Using only the crash stack, STAR starts from the crash point and goes backward towards the entry point of the program.
During the backward process, STAR computes the required condition to reach the crash point using an SMT (Satisfiability Modulo Theories) solver named Yices \cite{Dutertre2006}.
The objects that satisfy the required conditions are generated and orchestrated inside a JUnit test case. The test is run and the resulting crash stack is compared to the original one. If both match, the bug is said to be reproduced. When applied to different systems, STAR achieved 60\% accuracy. \\

Jaygarl et al. \cite{Jaygarl} created OCAT (Object Capture based Automated Testing).
The authors' approach starts by capturing objects created by the program when it runs on-field in order to provide them in an automated test process. Indeed the coverage of automated tests is often low due to the lack of correctly constructed objects.
Also, the objects can be mutated by means of evolutionary algorithms. These mutations target primitive fields in order to create even more objects and therefore improve the code coverage once more.
While not targeting the reproduction of a bug, OCAT is a well-known approach and was used as the main mechanism for bug reproduction. \\

Narayanasamy et al. \cite{Narayanasamy2005} proposed BugNet, a tool that continuously records program execution for deterministic replay debugging. According to the authors, the size of the recorded data needed to reproduce a bug with high accuracy is around 10MB. This recording is then sent to the developers and allows the deterministic replay of a bug. The authors argued that, with nowadays Internet bandwidth, the size of the recording is not an issue during the transmission of the recorded data, however, the instrumentation of the system is problematic since it slows down considerably the execution.\\

Jin et al. \cite{Jin2012} proposed BugRedux for reproducing field failures for in-house debugging. The tool aims to synthesize in-house executions that mimic field failures. To do so, the authors use several types of data collected in the field such as stack traces, crash stacks, and points of failure.
The data that successfully reproduced the field crash is sent to software developers to fix the bug. \\

Based on the success of BugRedux, the authors built F3 (Fault localization for Field Failures) \cite{Jin2013}.
F3 performs many executions of a program on top of BugRedux in order to cover different paths leading to the fault. It then generates many ‘pass’ and ‘fail’ paths which can lead to a better understanding of the bug. They also use grouping, profiling and filtering, to improve the fault localization process.\\

While being close to our approach, BugRedux and F3 may require the call sequence and/or the complete execution trace in order to achieve bug reproduction. When using only the crash traces (referred to as call stack at crash time in their paper), the success rate of BugRedux significantly drops to 37.5\% (6/16). The call sequence and the complete execution trace required to reach 100\% of bug reproduction can only be obtained through instrumentation and with an overhead ranging from 1\% to 1066\%. \\

Clause et al. \cite{Clause2007} proposed a technique for enabling and supporting debugging of field failures.
They record the execution of the program on the client side and propose to compress the generated data to the minimal required size to ensure that the reproduction is feasible.
This compression is also performed on the client side. Moreover, the authors keep traces of all accessed documents in the operating system and also compress/reduce them to the minimal.
Overall, they are able to reproduce on-field bug using a file weighting ≈70Kb. The minimal execution paths triggering the failure are then sent to the developers who can replay the execution on a sandbox, simulating the client’s environment. While efficient, this approach suffers from severe security and privacy issues. \\

RECORE (REconstructing CORE dumps) is a tool proposed by Rossler et al. \cite{Rossler2013}.
It instruments Java bytecode to wrap every method in a try and catch block while keeping a quasi-null overhead.
The tool starts from the core dump and tries (with evolutionary algorithms) to reproduce the same dump by executing the programs many times.
The set of inputs responsible for the failure is generated when the generated dump matches the collected one.
ReCrash \cite{Artzi2008} is a tool that aims to make software failures reproducible by preserving object states.
It uses an in-memory stack, which contains every argument and object clone of the real execution in order to reproduce a crash via the automatic generation of unit test cases.
Unit test cases are used to provide hints to the developers on the buggy code. This approach suffers from overhead when they record everything (between 13\% to 64\% in some cases).
The authors also propose an alternative in which they record only the methods surrounding the crash.
For this to work, the crash has to occur at least once so they could use the information causing the crash to identify the methods surrounding it when (and if) it appears. \\

JRapture \cite{Steven2000} is a capture/replay tool for observation-based testing.
The tool captures execution of Java programs to replay it in-house.  To capture the execution of a Java program, the authors used their own version of the Java Virtual Machine (JVM) and employ a lightweight, transparent capture process. Using their own JVM allows one to capture any interactions between a Java program and the system, including GUI, file, and console inputs, and on replay, it presents each thread with exactly the same input sequence it saw during capture.
Unfortunately, they have to make their customer use their own JVM in order to support their approach, which limits the generalization of the approach to mass-market software.\\

Finally, Zamfir et al. \cite{Parnin2011} proposed ESD, an execution synthesis approach which automatically synthesizes failure execution using only the stack trace information. However, this stack trace is extracted from the core dump and may not always contain the components that caused the crash.\\

Except for STAR, approaches targeting the reproduction of field crashes require the instrumentation of the code or the running platform in order to save the stack call or the objects to successfully reproduce bugs. As we discussed earlier, instrumentation can cause a massive overhead (1\% to 1066\%) while running the system. In addition, data generated at run-time using instrumentation may contain sensitive information.

\section{Issue and source code relationships\label{rel:issue-rela}}

Researchers started studying the relationships between issues and source code repositories more than two decades ago.
To the best of our knowledge the first ones who conduct this type of study on a significant scale were Perry and Stieg \cite{PerryDewayneE.1993}.
In these two decades, many aspects of these relationships have been studied in length.
For example, researchers  interested themselves in ameliorating the issues report by specifying guidelines to make a good report \cite{Bettenburg2008} and try to further simplify the existing models \cite{Herraiz2008}.

Then, we can find approaches on how long it will take for an issue to get fixed \cite{Bhattacharya2011,Zhang2013,Saha2014} and where it should be fixed \cite{Zhou2012,Kim2013a}.
With the rapidly increasing number of issues, the community also interested itself in prioritizing the issues report compared to one another \cite{Kim2011c} and do so by predicting the severity of an issue \cite{Lamkanfi2010}.

Finally, researchers proposed approaches to predict which issues will get reopened \cite{Zimmermann2012,Lo2013} which issues report is a duplicate of which other one \cite{Jalbert2008,Bettenburg2008a,Tian2012a}.

Another field of study consists in assigning these issues reports, if possible automatically to the right developers through triaging  \cite{Anvik2006,Jeong2009,Tamrawi2011a,Bortis2013}
and predicting which locations are likely to yield new bugs \cite{Kim2006,Kim2007}.

\section{Crash Prediction}

Predicting crash, fault and bug is very large and popular research area.
The main goal behind the plethora of papers is to save on manpower---being the most expensive resource to build software---by directing their efforts on locations likely to contain a bug, fault or crash.

There are two distinct trends in crash, fault and bug prediction in the papers accepted to major venues such as MSR, ICSE, ICSME and ASE:  history analysis and current version analysis.

In the history analysis, researchers extract and interpret information from  the system.
The idea being that the files or locations that are the most frequently changed are more likely to contain a bug.
Additionally, some of these approaches also assume that locations linked to a previous bug are likely to be linked to a bug in the future.

On the other hand, approaches using only the current version to predict bugs assume that the current version, i.e. its design, call graph, quality metrics and more, will trigger the appearance of the bug in the future.
Consequently, they do no require the history and only need the current source-code.

In the remaining of this section, we will describe approaches belonging to the two families.

\subsubsection{Change logs approaches}
\label{subs:Change logs approaches}

Change logs based approaches rely on mining the historical data of the application and more particularly, the source code \textit{diffs}.
A source code \textit{diffs} contains two versions of the same code in one file.
Indeed, it contains the lines of code that have been deleted and the one that has been added.
It is worth noting that, \textit{diffs} files do not represent the concept of modified line.
Indeed, a modified line will be represented by a deletion and an addition.
Researchers mainly use five metrics when dealing with \textit{diffs} files:

\begin{itemize}
  \item Number of files: The number of modified files in a given commit
  \item Insertions: The number of added lines
  \item Deletions: The number of deleted lines
  \item Churns: The number of deleted lines immediately followed by an insertion which give an approximation of how many lines have been modified
  \item Hunks: The number of consecutive blocks of lines. This gives an approximation of how many distinct locations have been edited to accomplish a unit of work.
\end{itemize}

Naggapan \textit{et al.} studied the churns metric and how it can be connected to the apparition of new defect in a complex software systems.
They established that relative churns are, in fact, a better metric than classical churn \cite{Nagappan} while studying Windows Server 2003.

Hassan, interested himself with the entropy of change, i.e. how complex the change is \cite{Hassan2009}.
Then, the complexity of the change, or entropy, can be used to predict bugs.
The more complex a change is, the more likely it is to bring the defect with it.
Hassan used its entropy metric, with success, on six different systems.
Prior to this work, Hassan, in collaboration with Holt proposed an approach that highlights the top ten most susceptible locations to have a bug using heuristics based on \textit{diffs} file metrics \cite{Hassan2005}.
Moreover, their heuristics also leverage the data of the bug tracking system.
Indeed, they use the past defect location to predict new ones.
The conclusion of these two approaches has been that recently modified and fixed locations where the most defect-prone compared to frequently modified ones.

Similarly to Hassan and Hold,  Ostrand \textit{et al.} predict future crash location by combining the data from changed and past defect locations \cite{Ostrand2005}.
The main difference between Hassan and Hold and Ostrand \textit{et al.} is that Ostrand \textit{et al.} validate their approach on industrial systems as they are members of the AT\&T lab while Hassan and Hold validated their approach on open-source systems.
This proved that these metrics are relevant for open-source and industrial systems.

Kim \textit{et al.} applied the same recipe and mined recent changes and defects with their approach named bug cache \cite{Kim2007a}.
However, they are more accurate than the previous approaches at detecting defect location by taking into account that is more likely for a developer to make a change that introduces a defect when being under pressure.
Such changes can be pushed to revision-control system when deadlines and releases date are approaching.

\subsubsection{Single-version approaches}

Approaches belonging to the single-version family will only consider the current version of the software at hand.
Simply put, they don't leverage the history of changes or bug reports.
Despite this fact, that one can see as a disadvantage compared to approaches that do leverage history, these approaches yield interesting results using code-based metrics.

Chidamber and Kemerer published the well-known CK metrics suite \cite{Chidamber1994} for object oriented designs and inspired Moha \textit{et al.} to publish similar metrics for service oriented programs \cite{Moha}.
Another famous metric suite for assessing the quality of a given software design is Briand's coupling metrics \cite{Briand1999a}.

The CK and Briand's metrics suites have been used, for example, by Basili \textit{et al.} \cite{Basili1996}, El Emam \textit{et al.} \cite{ElEmam2001},  Subramanyam \textit{et al.} \cite{Subramanyam2003} and Gyimothy \textit{et al.} \cite{Gyimothy2005} for object oriented designs.
Service oriented designs have been far less studied than object oriented design as they are relatively new, but, Nayrolles \textit{et al.} \cite{Nayrolles,Nayrolles2013d}, Demange \textit{et al.} \cite{demange2013} and Palma \textit{et al.} \cite{Palma2013} used Moha et \textit{et al.} metric suites to detect software defects.

All these approaches, proved software metrics to be useful at detecting software fault for object oriented and service oriented designs, respectively.

Finally, Nagappan \textit{et al.} \cite{Nagappan2005,Nagappan2006} and Zimmerman \cite{Zimmermann2007,Zimmermann2008} further refined metrics-based detection by using statical analysis and call-graph analysis.

\\
While hundreds of bug prediction papers have been published by academia over the last decade, the developed tools and approaches fail to change developer behavior while deployed in industrial environment \cite{Lewis2013}.
This is mainly due to the lack of actionable message, i.e. messages that provide concrete steps to resolve the problem at hand.

{\tt pErICOPE} (Ecosystem Improve source COde during Programming session with real-time mining of common knowlEdge), our proposed ecosystem, will be different in a sense that we will provide actionable messages as presented in Section \ref{sec:objective-thesis}.


\section{Clone Detection}
\label{sec:rel-clones}

Clone detection is an important and difficult task. Throughout the years, researchers and practitioners have developed a considerable number of methods and tools in order to detect efficiently source code clones.

Text-based techniques use the code --- often raw (e.g. with comments) --- and compare sequences of code (blocks) to each other in order to identify potential clones. Johnson was perhaps the first one to use fingerprints to detect clones\cite{Johnson1993,Johnson1994}. Blocks of code are hashed, producing fingerprints that can be compared.
If two blocks share the same fingerprint, they are considered as clones.
Manber et al. \cite{Manber1994} and Ducasse et al.\cite{Ducasse1999} refined the fingerprint technique by using leading keywords and dot-plots.

Tree-matching and metric-based are two sub-categories of syntactic analysis for clone detection.
Syntactic analysis consists of building abstract syntax trees (AST) and analyse them with a set of dedicated metrics or searching for identical sub-trees.
Many approaches using AST have been published using sub-tree comparison including the work of Baxter et al.\cite{Baxter1998}, Wahleret et al. \cite{Wahler}, or more recently, the work of Jian et al. with Deckard \cite{Jiang2007}.
An AST-based approach compares metrics computed on the AST, rather than the code itself, to identify clones \cite{Patenaude1999, Balazinska}.

Another approach to detect clones is to use static analysis and to leverage the semantics of the program to improve the detection.
These techniques rely on program dependency graphs where nodes are statements and edges are dependencies.
Then, the problem of finding clones is reduced to the problem of finding identical sub-groups in the program dependency graph.
Examples of recent techniques that fall into this category are the ones presented by Krinke et al.\cite{Krinke2001} and  Gabel et al. \cite{Gabel2008}.

Many clone detection tools have been created using a lexical approach for clone detection. Here, the code is transformed into a series of tokens. If sub-series repeat themselves, it means that a potential clone is in the code. Some popular tools that use this technique include, but not limited to, Dup\cite{Baker}, CCFinder\cite{Kamiya2002}, and CP-Miner\cite{Li2006}.

Furthermore, a large number of taxonomies have been published in an attempt to classify  clones and ease the research on clone detection\cite{Mayrand1996,Balazinska1999,Koschke2006,Bellon2007,Kontogiannis,Kapser}.

Other active research activities in clone detection focus on clone removal and management. Once detected, an obvious step is to provide approaches to remove clones in an automatic way or (at least) keep track of them if removing them is not an option.
Most modern IDEs provide the \textit{extract method} feature that transforms a potentially copy-pasted block of code into a method and a call to the newly generated method\cite{Komondoor,higo2004refactoring}.
More advanced techniques (see Codelink\cite{Toomim} and\cite{Duala-Ekoko2007}) involve analysing the output of CCFinder\cite{Kamiya2002a,Livieri2007} or program dependencies graphs\cite{higo2004refactoring} to automatically suggest a method that would go through the \textit{extract method} process.

The aforementioned  techniques, however, focus on detecting clones after they are inserted in the code. Only a few studies focus on preventing the insertion of clones. Lague et al. \cite{Lague} conducted a very large empirical study with 10,000 developers over 3 years, where developers where asked to use clone detection tools during the development process of a very large telecom system. The authors found that while clones are being removed over time, using clone detection tools help improving the quality of the system as it prevents defects to reach the customers. Duala et al. \cite{Duala-Ekoko2007,Duala-Ekoko2010} proposed to create clone region descriptors (CRDs), which describe clone regions within methods in a robust way that is independent from the exact text of the clone region or its location in a file. Then, using CRDs, clone insertion can be prevented.

PRECINCT aims to prevent clone insertion while integrating the clone detection process in a transparent manner in the day-to-day development process. This way, software developers do not have to resort to external tools to remove clones after they are inserted. Our approach operates at commit time, notifying software developers of possible clones as they commit their code.
